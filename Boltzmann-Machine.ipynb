{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook is based on [Quantum Boltzmann Machine (M. H. Amin, et al., 2018)](https://journals.aps.org/prx/abstract/10.1103/PhysRevX.8.021050).\r\n",
    "\r\n",
    "Code is referenced from [mareksubocz/QRBM](https://github.com/mareksubocz/QRBM)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import torch\r\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "class RandomVectorDataset(Dataset):\r\n",
    "    \"\"\"\r\n",
    "    Create a random boolean vector dataset with multiple modes, each centered at a random centerpoint with success probability p.\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, N, modes, p, dataset_size, sd=None):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        np.random.seed(sd)\r\n",
    "        center_lst = []\r\n",
    "        dataset_lst = []\r\n",
    "        for mode in range(modes):\r\n",
    "            s = np.random.binomial(1, 0.5, N)\r\n",
    "            center_lst.append(s)\r\n",
    "            # dataset_lst.append([])\r\n",
    "            for _ in range(dataset_size):\r\n",
    "                data = (np.random.binomial(1, p, N) == s)\r\n",
    "                # dataset_lst[-1].append(data)\r\n",
    "                dataset_lst.append(data)\r\n",
    "\r\n",
    "        self.centers = center_lst\r\n",
    "        self.samples = dataset_lst\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.samples)\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        return self.samples[idx]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "N = 5\r\n",
    "modes = 2\r\n",
    "p = 0.9\r\n",
    "dataset_size = 100\r\n",
    "seed = 0\r\n",
    "\r\n",
    "dataset = RandomVectorDataset(N, modes, p, dataset_size, seed)\r\n",
    "dataset.centers"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([1, 1, 1, 1, 0]), array([1, 1, 0, 0, 0])]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True, num_workers=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "for i, batch in enumerate(dataloader):\r\n",
    "    print(i, batch)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 tensor([[ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False]])\n",
      "1 tensor([[ True, False,  True, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True, False,  True, False, False]])\n",
      "2 tensor([[ True,  True, False,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [ True, False,  True,  True,  True],\n",
      "        [ True,  True, False, False,  True],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True, False, False,  True],\n",
      "        [ True,  True, False, False,  True]])\n",
      "3 tensor([[ True,  True, False, False, False],\n",
      "        [False,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False]])\n",
      "4 tensor([[ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False,  True],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False]])\n",
      "5 tensor([[ True,  True,  True,  True, False],\n",
      "        [False,  True, False, False,  True],\n",
      "        [ True,  True, False,  True, False],\n",
      "        [False,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True, False,  True,  True, False],\n",
      "        [ True,  True, False, False,  True],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False]])\n",
      "6 tensor([[False, False,  True,  True, False],\n",
      "        [ True,  True, False,  True, False],\n",
      "        [False,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [False,  True,  True, False,  True]])\n",
      "7 tensor([[ True,  True,  True, False, False],\n",
      "        [False,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [False,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False]])\n",
      "8 tensor([[False,  True, False, False,  True],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True, False,  True, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [False, False,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False]])\n",
      "9 tensor([[ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False]])\n",
      "10 tensor([[ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True, False, False,  True],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False]])\n",
      "11 tensor([[ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False,  True, False],\n",
      "        [False,  True, False, False,  True],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False,  True]])\n",
      "12 tensor([[False,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False,  True],\n",
      "        [False,  True, False,  True, False],\n",
      "        [False,  True, False, False, False],\n",
      "        [ True,  True, False,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True,  True, False, False, False]])\n",
      "13 tensor([[ True,  True,  True, False, False],\n",
      "        [False,  True,  True,  True, False],\n",
      "        [ True, False,  True,  True,  True],\n",
      "        [ True, False,  True,  True, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False,  True],\n",
      "        [ True,  True,  True,  True, False]])\n",
      "14 tensor([[False,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False,  True],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [False, False, False,  True, False]])\n",
      "15 tensor([[ True,  True, False,  True, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True, False,  True],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [False,  True,  True,  True, False],\n",
      "        [ True,  True, False,  True,  True],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False,  True],\n",
      "        [ True,  True, False, False, False]])\n",
      "16 tensor([[ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True, False,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [False,  True,  True, False, False]])\n",
      "17 tensor([[ True,  True,  True,  True, False],\n",
      "        [ True,  True, False,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True, False,  True,  True, False],\n",
      "        [ True, False,  True,  True,  True],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False]])\n",
      "18 tensor([[ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [False,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [False,  True, False, False, False]])\n",
      "19 tensor([[ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True, False, False,  True],\n",
      "        [ True,  True, False,  True, False]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "qbm1 = QBM(5, 2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "qbm1.train(dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-4.02431977 -0.57270882] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.98 0.96]\n",
      "[-4.02431977 -0.57270882] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.98 1.  ]\n",
      "[-2.54902838 -0.93878657] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1.   1.   0.   0.   0.   1.   0.98]\n",
      "[-2.82182014 -0.245177  ] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[0.  1.  1.  1.  0.  0.9 0.7]\n",
      "[-2.54902838 -0.93878657] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1. 1. 0. 0. 0. 1. 1.]\n",
      "[-1.34652875 -0.61125475] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[0. 1. 0. 0. 0. 1. 1.]\n",
      "[-4.02431977 -0.57270882] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.94 1.  ]\n",
      "[-2.54902838 -0.93878657] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.98 1.  ]\n",
      "[-4.02431977 -0.57270882] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.98 0.98]\n",
      "[-4.02431977 -0.57270882] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1. 1. 1. 1. 0. 1. 1.]\n",
      "[-4.02431977 -0.57270882] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1.   1.   1.   1.   0.   1.   0.96]\n",
      "[-3.31590761  0.10331011] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1.   1.   0.   0.   1.   0.68 0.42]\n",
      "[-2.82182014 -0.245177  ] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[0.   1.   1.   1.   0.   0.72 0.58]\n",
      "[-3.31590761  0.10331011] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1.   1.   0.   0.   1.   0.56 0.46]\n",
      "[-4.02431977 -0.57270882] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.98 0.98]\n",
      "[-2.54902838 -0.93878657] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.98 0.98]\n",
      "[-3.43793283 -0.73694756] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1. 0. 1. 1. 0. 1. 1.]\n",
      "[-4.02431977 -0.57270882] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.96 0.96]\n",
      "[-4.20481206  0.30514912] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1.   0.   1.   1.   1.   0.28 0.54]\n",
      "[-4.791199    0.46938786] [[0.         0.15270524]\n",
      " [0.15270524 0.        ]]\n",
      "[1.   1.   1.   1.   1.   0.4  0.14]\n",
      "[-2.75642385 -0.6105918 ] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   0.   1.   0.   0.98 1.  ]\n",
      "[-3.46243977  0.01851918] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.56 0.44]\n",
      "[-2.75642385 -0.6105918 ] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   0.   1.   0.   1.   0.98]\n",
      "[-2.19350838 -0.56474257] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.94 0.96]\n",
      "[-2.19350838 -0.56474257] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.88 0.96]\n",
      "[-3.46243977  0.01851918] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.48 0.5 ]\n",
      "[-2.75642385 -0.6105918 ] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   0.   1.   0.   0.98 1.  ]\n",
      "[-2.92606761  0.50155411] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   0.   0.   1.   0.4  0.46]\n",
      "[-2.75642385 -0.6105918 ] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   0.   1.   0.   0.98 0.94]\n",
      "[-3.46243977  0.01851918] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.32 0.72]\n",
      "[-2.8995243   0.06436841] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   1.   0.   0.   0.4  0.68]\n",
      "[-2.40184014  0.198651  ] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[0.   1.   1.   1.   0.   0.64 0.3 ]\n",
      "[-3.46243977  0.01851918] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.  1.  1.  1.  0.  0.4 0.7]\n",
      "[-3.46243977  0.01851918] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.5  0.54]\n",
      "[-3.46243977  0.01851918] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.58 0.48]\n",
      "[-2.75642385 -0.6105918 ] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   0.   1.   0.   0.96 0.9 ]\n",
      "[-2.19350838 -0.56474257] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.98 0.92]\n",
      "[-2.8995243   0.06436841] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   1.   0.   0.   0.46 0.6 ]\n",
      "[-2.40184014  0.198651  ] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[0.   1.   1.   1.   0.   0.5  0.52]\n",
      "[-3.46243977  0.01851918] [[0.         0.23202404]\n",
      " [0.23202404 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.64 0.5 ]\n",
      "[-2.68642161  1.08492811] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1.   1.   0.   0.   1.   0.04 0.06]\n",
      "[-1.93582238 -0.02206857] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.42 0.64]\n",
      "[-3.03376977  0.72218918] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.32 0.24]\n",
      "[-1.93582238 -0.02206857] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.54 0.56]\n",
      "[-2.6264383   0.74410241] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1.   1.   1.   0.   0.   0.28 0.38]\n",
      "[-3.03376977  0.72218918] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.24 0.26]\n",
      "[-1.7585392   0.17963426] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[0.   0.   1.   1.   0.   0.46 0.54]\n",
      "[-1.93582238 -0.02206857] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.68 0.3 ]\n",
      "[-2.68642161  1.08492811] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1.   1.   0.   0.   1.   0.06 0.08]\n",
      "[-3.03376977  0.72218918] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.28 0.38]\n",
      "[-3.784369    1.82918586] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1. 1. 1. 1. 1. 0. 0.]\n",
      "[-1.93582238 -0.02206857] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.72 0.3 ]\n",
      "[-2.68642161  1.08492811] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1.   1.   0.   0.   1.   0.02 0.08]\n",
      "[-3.37703753  1.85109908] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1. 1. 1. 0. 1. 0. 0.]\n",
      "[-2.30039936  0.20788767] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1.   0.   1.   0.   0.   0.74 0.32]\n",
      "[-1.93582238 -0.02206857] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.36 0.64]\n",
      "[-2.6264383   0.74410241] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1.   1.   1.   0.   0.   0.34 0.22]\n",
      "[-0.98663075 -0.02840875] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[0.   1.   0.   0.   0.   0.28 0.66]\n",
      "[-1.93582238 -0.02206857] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.62 0.44]\n",
      "[-3.45833006  1.29297112] [[0.         0.30954764]\n",
      " [0.30954764 0.        ]]\n",
      "[1.   0.   1.   1.   1.   0.02 0.  ]\n",
      "[-2.93342777  1.01767118] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.18 0.06]\n",
      "[-2.93342777  1.01767118] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.16 0.16]\n",
      "[-2.93342777  1.01767118] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.18 0.08]\n",
      "[-1.89452838  0.24868543] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.44 0.56]\n",
      "[-2.93342777  1.01767118] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.18 0.08]\n",
      "[-1.89452838  0.24868543] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.52 0.46]\n",
      "[-2.67535561  1.39321411] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1. 1. 0. 0. 1. 0. 0.]\n",
      "[-1.89452838  0.24868543] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.38 0.7 ]\n",
      "[-1.89452838  0.24868543] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.42 0.6 ]\n",
      "[-2.5957923   1.08494841] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   1.   1.   0.   0.   0.26 0.08]\n",
      "[-1.58556144 -0.38952131] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   0.   0.   0.   0.   0.38 0.7 ]\n",
      "[-2.28682536  0.44674167] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   0.   1.   0.   0.   0.68 0.26]\n",
      "[-1.58556144 -0.38952131] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   0.   0.   0.   0.   0.7  0.52]\n",
      "[-1.58556144 -0.38952131] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   0.   0.   0.   0.   0.52 0.58]\n",
      "[-2.93342777  1.01767118] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.16 0.1 ]\n",
      "[-1.89452838  0.24868543] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.74 0.32]\n",
      "[-1.89452838  0.24868543] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.48 0.52]\n",
      "[-0.97604875  0.15333325] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[0.   1.   0.   0.   0.   0.62 0.32]\n",
      "[-3.40528806  1.52399312] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   0.   1.   1.   1.   0.   0.02]\n",
      "[-2.23216385  0.1814082 ] [[0.         0.34140364]\n",
      " [0.34140364 0.        ]]\n",
      "[1.   1.   0.   1.   0.   0.56 0.44]\n",
      "[-1.83292838  0.52280543] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.32 0.62]\n",
      "[-0.95558875  0.32185325] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[0.   1.   0.   0.   0.   0.02 0.52]\n",
      "[-1.83292838  0.52280543] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.48 0.42]\n",
      "[-3.647155    2.48753586] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1. 1. 1. 1. 1. 0. 0.]\n",
      "[-2.64719561  1.70266611] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1.   1.   0.   0.   1.   0.   0.02]\n",
      "[-2.4847599  2.3823289] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[0. 1. 1. 0. 1. 0. 0.]\n",
      "[-1.83292838  0.52280543] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.54 0.46]\n",
      "[-2.83288777  1.30767518] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.02 0.02]\n",
      "[-1.83292838  0.52280543] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.66 0.26]\n",
      "[-3.647155    2.48753586] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1. 1. 1. 1. 1. 0. 0.]\n",
      "[-1.83292838  0.52280543] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.52 0.24]\n",
      "[-1.83292838  0.52280543] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.52 0.34]\n",
      "[-2.5478323   1.40342041] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1.   1.   1.   0.   0.   0.06 0.04]\n",
      "[-2.83288777  1.30767518] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.04 0.06]\n",
      "[-1.83292838  0.52280543] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.38 0.6 ]\n",
      "[-2.5478323   1.40342041] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1.   1.   1.   0.   0.   0.04 0.06]\n",
      "[-2.83288777  1.30767518] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.02 0.08]\n",
      "[-1.83292838  0.52280543] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.8  0.18]\n",
      "[-1.83292838  0.52280543] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.62 0.26]\n",
      "[-2.11798385  0.4270602 ] [[0.         0.37718224]\n",
      " [0.37718224 0.        ]]\n",
      "[1.   1.   0.   1.   0.   0.66 0.32]\n",
      "[-1.68238238  0.65280343] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.38 0.54]\n",
      "[-2.67222177  1.44581318] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.   0.02]\n",
      "[-2.3961863   1.53803841] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1.   1.   1.   0.   0.   0.   0.04]\n",
      "[-2.67222177  1.44581318] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.   0.06]\n",
      "[-2.67222177  1.44581318] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.04 0.04]\n",
      "[-1.14303022  0.31479002] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[0.   1.   0.   1.   0.   0.34 0.5 ]\n",
      "[-0.91858728 -0.46126872] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[0.   0.   0.   1.   0.   0.4  0.44]\n",
      "[-2.77303708  1.74136288] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1. 1. 0. 1. 1. 0. 0.]\n",
      "[-3.486841    2.62659786] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1. 1. 1. 1. 1. 0. 0.]\n",
      "[-2.49700161  1.83358811] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1.   1.   0.   0.   1.   0.02 0.02]\n",
      "[-3.21080553  2.71882308] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1. 1. 1. 0. 1. 0. 0.]\n",
      "[-0.86699475  0.40701525] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[0.   1.   0.   0.   0.   0.24 0.38]\n",
      "[-1.68238238  0.65280343] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.68 0.16]\n",
      "[-1.68238238  0.65280343] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.34 0.6 ]\n",
      "[-1.68238238  0.65280343] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.36 0.44]\n",
      "[-1.68238238  0.65280343] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.32 0.56]\n",
      "[-2.67222177  1.44581318] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.04 0.06]\n",
      "[-2.67222177  1.44581318] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.08 0.02]\n",
      "[-1.68238238  0.65280343] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.52 0.28]\n",
      "[-1.68238238  0.65280343] [[0.         0.38790504]\n",
      " [0.38790504 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.34 0.6 ]\n",
      "[-1.52846066  1.39393422] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[0.   1.   1.   0.   0.   0.02 0.  ]\n",
      "[-1.59713238  0.78898343] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.36 0.14]\n",
      "[-2.57531177  1.59717318] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.   0.04]\n",
      "[-2.57531177  1.59717318] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.02 0.02]\n",
      "[-2.3112883   1.68108241] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1.   1.   1.   0.   0.   0.02 0.02]\n",
      "[-2.57531177  1.59717318] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1. 1. 1. 1. 0. 0. 0.]\n",
      "[-2.57531177  1.59717318] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.02 0.02]\n",
      "[-1.59713238  0.78898343] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.3  0.42]\n",
      "[-2.3112883   1.68108241] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1.   1.   1.   0.   0.   0.02 0.  ]\n",
      "[-1.41162944 -0.03811531] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1.   0.   0.   0.   0.   0.26 0.76]\n",
      "[-2.57531177  1.59717318] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.   0.02]\n",
      "[-1.59713238  0.78898343] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.5  0.26]\n",
      "[-2.3112883   1.68108241] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1. 1. 1. 0. 0. 0. 0.]\n",
      "[-1.59713238  0.78898343] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.48 0.18]\n",
      "[-1.59713238  0.78898343] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.38 0.26]\n",
      "[-2.57531177  1.59717318] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.02 0.  ]\n",
      "[-2.41293961  1.97280411] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1. 1. 0. 0. 1. 0. 0.]\n",
      "[-2.38980883  0.77007444] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1.   0.   1.   1.   0.   0.34 0.46]\n",
      "[-1.6069812   0.48292626] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[0.   0.   1.   1.   0.   0.36 0.6 ]\n",
      "[-1.59713238  0.78898343] [[0.         0.39837924]\n",
      " [0.39837924 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.44 0.34]\n",
      "[-1.50818638  0.86129743] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.34 0.16]\n",
      "[-2.2140263   1.76620041] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[1. 1. 1. 0. 0. 0. 0.]\n",
      "[-1.76314585  0.7903242 ] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[1.   1.   0.   1.   0.   0.48 0.34]\n",
      "[-2.32540161  2.04476611] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[1. 1. 0. 0. 1. 0. 0.]\n",
      "[-1.50818638  0.86129743] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.2  0.44]\n",
      "[-2.46898577  1.69522718] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.02 0.02]\n",
      "[-0.60053181 -0.30359349] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[0.   0.   0.   0.   0.   0.08 0.  ]\n",
      "[-1.46419866  1.44715222] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[ 0.    1.    1.    0.    0.    0.02 -0.02]\n",
      "[-1.50818638  0.86129743] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.28 0.26]\n",
      "[-1.50818638  0.86129743] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.26 0.34]\n",
      "[-2.46898577  1.69522718] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.02 0.02]\n",
      "[-2.46898577  1.69522718] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.   0.02]\n",
      "[-1.57557398  1.72571793] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[ 0.    1.    0.    0.    1.   -0.02 -0.02]\n",
      "[-1.50818638  0.86129743] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.26 0.42]\n",
      "[-2.46898577  1.69522718] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.02 0.  ]\n",
      "[-2.46898577  1.69522718] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.02 0.  ]\n",
      "[-0.75835875  0.54224925] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[0.   1.   0.   0.   0.   0.04 0.02]\n",
      "[-2.46898577  1.69522718] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.   0.02]\n",
      "[-1.57557398  1.72571793] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[ 0.    1.    0.    0.    1.   -0.02  0.  ]\n",
      "[-1.50818638  0.86129743] [[0.         0.40487144]\n",
      " [0.40487144 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.4  0.14]\n",
      "[-2.1394023   1.82120041] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1.   1.   1.   0.   0.   0.   0.02]\n",
      "[-2.38503377  1.75308718] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1. 1. 1. 1. 0. 0. 0.]\n",
      "[-1.43149438  0.91387743] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.5  0.16]\n",
      "[-1.67712585  0.8457642 ] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1.   1.   0.   1.   0.   0.4  0.32]\n",
      "[-1.29760344  0.04339469] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1.  0.  0.  0.  0.  0.4 0.6]\n",
      "[-1.43149438  0.91387743] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.22 0.34]\n",
      "[-1.43149438  0.91387743] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.28 0.38]\n",
      "[-2.38503377  1.75308718] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.02 0.  ]\n",
      "[-1.65496214  1.406979  ] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[0.  1.  1.  1.  0.  0.1 0. ]\n",
      "[-3.209817    2.94029586] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1. 1. 1. 1. 1. 0. 0.]\n",
      "[-2.1394023   1.82120041] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1.   1.   1.   0.   0.   0.02 0.  ]\n",
      "[-2.1394023   1.82120041] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1. 1. 1. 0. 0. 0. 0.]\n",
      "[-2.38503377  1.75308718] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.02 0.04]\n",
      "[-2.1394023   1.82120041] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1.   1.   1.   0.   0.   0.02 0.  ]\n",
      "[-2.38503377  1.75308718] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.02 0.  ]\n",
      "[-2.38503377  1.75308718] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1. 1. 1. 1. 0. 0. 0.]\n",
      "[-2.25114283  0.88260444] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1.   0.   1.   1.   0.   0.24 0.38]\n",
      "[-1.43149438  0.91387743] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.12 0.44]\n",
      "[-1.43149438  0.91387743] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.2  0.36]\n",
      "[-1.43149438  0.91387743] [[0.         0.41033624]\n",
      " [0.41033624 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.12 0.46]\n",
      "[-1.35607838  0.99811543] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.06 0.18]\n",
      "[-2.29597777  1.85030518] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1. 1. 1. 1. 0. 0. 0.]\n",
      "[-2.29597777  1.85030518] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1. 1. 1. 1. 0. 0. 0.]\n",
      "[-1.58938985  0.9377022 ] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1.  1.  0.  1.  0.  0.3 0.2]\n",
      "[-1.35607838  0.99811543] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.26 0.28]\n",
      "[-2.29597777  1.85030518] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1. 1. 1. 1. 0. 0. 0.]\n",
      "[-1.58938985  0.9377022 ] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1.   1.   0.   1.   0.   0.26 0.1 ]\n",
      "[-1.35607838  0.99811543] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.34 0.12]\n",
      "[-1.35607838  0.99811543] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.26 0.14]\n",
      "[-2.29597777  1.85030518] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1. 1. 1. 1. 0. 0. 0.]\n",
      "[-2.29597777  1.85030518] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.02 0.  ]\n",
      "[-2.29597777  1.85030518] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.02 0.  ]\n",
      "[-1.35607838  0.99811543] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.36 0.04]\n",
      "[-2.29597777  1.85030518] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1. 1. 1. 1. 0. 0. 0.]\n",
      "[-1.35607838  0.99811543] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.28 0.12]\n",
      "[-2.29597777  1.85030518] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1.   1.   1.   1.   0.   0.02 0.02]\n",
      "[-2.18649361  2.18602811] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1. 1. 0. 0. 1. 0. 0.]\n",
      "[-1.47987398  1.80080393] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[ 0.    1.    0.    0.    1.    0.02 -0.04]\n",
      "[-2.29597777  1.85030518] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1. 1. 1. 1. 0. 0. 0.]\n",
      "[-1.35607838  0.99811543] [[0.         0.41564484]\n",
      " [0.41564484 0.        ]]\n",
      "[1.   1.   0.   0.   0.   0.18 0.16]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "qbm1.get_weights()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[ 0.        ,  0.32600774,  0.3733156 ,  0.1208794 , -0.2788038 ,\n",
       "         -0.34279382,  0.20070809],\n",
       "        [ 0.32600774,  0.        , -0.24599864,  0.50692582,  0.56106582,\n",
       "         -0.04404347,  0.45653737],\n",
       "        [ 0.3733156 , -0.24599864,  0.        ,  1.18332303,  0.33175738,\n",
       "         -0.35331596,  0.45649949],\n",
       "        [ 0.1208794 ,  0.50692582,  1.18332303,  0.        ,  0.43304639,\n",
       "         -0.11289374, -0.02853461],\n",
       "        [-0.2788038 ,  0.56106582,  0.33175738,  0.43304639,  0.        ,\n",
       "         -0.41791362,  0.59444034],\n",
       "        [-0.34279382, -0.04404347, -0.35331596, -0.11289374, -0.41791362,\n",
       "          0.        ,  0.41812644],\n",
       "        [ 0.20070809,  0.45653737,  0.45649949, -0.02853461,  0.59444034,\n",
       "          0.41812644,  0.        ]]),\n",
       " array([-0.06578278,  1.43889732, -0.20320938,  1.46064206, -0.04972262,\n",
       "        -0.50450181, -0.28280349]))"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "qbm1.sample(qbm1.w, qbm1.b, num=1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0. , -0.4, -0.4,  0.4, -0.2, -0.2,  0.2])"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quantum Boltzmann Machine"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "class QBM:\r\n",
    "    \"\"\"\r\n",
    "    A general quantum Boltzmann machine class. No restrictions on the connection between nodes.\r\n",
    "    Variables are binary (\\pm 1).\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self,\r\n",
    "                 n_vis,\r\n",
    "                 n_hid,\r\n",
    "                 sampler='QMC',\r\n",
    "                 Gamma=0.1,\r\n",
    "                 err_function='mse',\r\n",
    "                 use_tqdm=True,\r\n",
    "                 tqdm=None):\r\n",
    "\r\n",
    "        if err_function not in {'mse', 'cosine'}:\r\n",
    "            raise ValueError('err_function should be either \\'mse\\' or \\'cosine\\'')\r\n",
    "\r\n",
    "        # progress bar\r\n",
    "        self._use_tqdm = use_tqdm\r\n",
    "        self._tqdm = None\r\n",
    "\r\n",
    "        if use_tqdm or tqdm is not None:\r\n",
    "            from tqdm import tqdm\r\n",
    "            self._tqdm = tqdm\r\n",
    "\r\n",
    "        self.n_vis = n_vis\r\n",
    "        self.n_hid = n_hid\r\n",
    "        self.n = n_vis + n_hid\r\n",
    "        self.sampler = sampler\r\n",
    "        self.Gamma = Gamma\r\n",
    "\r\n",
    "        # Convention:\r\n",
    "        # varibales 0 to (n_vis - 1) represent visible nodes;\r\n",
    "        # variables n_vis to (n - 1) represent hidden nodes;\r\n",
    "        # energy = -w_v.dot(visible).dot(visible) - w_h.dot(hidden).dot(hidden) - 2 * w_vh.dot(visible).dot(hidden)\r\n",
    "        #        = -w.dot(nodes).dot(nodes) - b.dot(nodes)\r\n",
    "        # self.w_v = np.random.uniform(-1, 1, (self.n_vis, self.n_vis))\r\n",
    "        # self.w_v = 0.5 * (self.w_v + self.w_v.T)\r\n",
    "        # self.w_h = np.random.uniform(-1, 1, (self.n_hid, self.n_hid))\r\n",
    "        # self.w_h = 0.5 * (self.w_h + self.w_h.T)\r\n",
    "        # self.w_vh = np.random.uniform(-1, 1, (self.n_vis, self.n_hid))\r\n",
    "        self.w = np.random.uniform(-1, 1, (self.n, self.n))\r\n",
    "        self.w = 0.5 * (self.w + self.w.T) # symmetric\r\n",
    "        np.fill_diagonal(self.w, 0)\r\n",
    "        self.b = np.random.uniform(-1, 1, self.n)\r\n",
    "\r\n",
    "        self.n_epoch = 0\r\n",
    "\r\n",
    "    def get_weights(self):\r\n",
    "        return self.w, self.b\r\n",
    "\r\n",
    "    def set_weights(self, w, b):\r\n",
    "        self.w = 0.5 * (w + w.T)\r\n",
    "        self.b = b\r\n",
    "\r\n",
    "    def get_Z(self):\r\n",
    "        \"\"\"\r\n",
    "        Return the canonical partition function\r\n",
    "        \"\"\"\r\n",
    "        Z = np.sum(np.exp(-self.energies))\r\n",
    "        self.Z = Z\r\n",
    "        return Z\r\n",
    "\r\n",
    "    def sample(self, J, h, num=10, Gamma0=5, M=10, T=0.05):\r\n",
    "        \"\"\"\r\n",
    "        Sample from the Boltzmann distribution of energy = -Q.dot(state).dot(state) and average them out\r\n",
    "        \"\"\"\r\n",
    "        if self.sampler == 'QMC' or self.sampler == 'SQA':\r\n",
    "            from sqa import one_SQA_run\r\n",
    "            \r\n",
    "            trans_fld_sched = np.linspace(Gamma0, self.Gamma, num=100)\r\n",
    "\r\n",
    "            results = []\r\n",
    "            for _ in range(num):\r\n",
    "                results.append(one_SQA_run(J, h, trans_fld_sched, M, T))\r\n",
    "            \r\n",
    "            return np.sum(np.reshape(np.array(results), (num * M, -1)), axis=0) / num / M\r\n",
    "\r\n",
    "    def train(self, training_dataset, epochs=1, batches=10, lr=0.11, lr_decay=0.1, epoch_drop=None):\r\n",
    "        \"\"\"\r\n",
    "        Minimizes the upper bound of likelihood function.\r\n",
    "        Updates after each batch. Runs over the whole dataset after each epoch.\r\n",
    "        \"\"\"\r\n",
    "        # learning_curve_plot = []\r\n",
    "        batch_size = len(training_dataset) // batches\r\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\r\n",
    "\r\n",
    "        # if epoch_drop is None:\r\n",
    "        #     epoch_drop = epochs // 5\r\n",
    "\r\n",
    "        # for batch in training_data.shuffle(len(training_data), reshuffle_each_iteration=True).repeat(epochs).batch(batch_size):\r\n",
    "        for batch in dataloader:\r\n",
    "            pos_phase_w = np.zeros((self.n, self.n))\r\n",
    "            pos_phase_b = np.zeros(self.n)\r\n",
    "            for data in batch: # parallelizable\r\n",
    "                # offset = self.w[:self.n_vis, :self.n_vis].dot(data).dot(data) + self.b[:self.n_vis].dot(data)\r\n",
    "                b_eff = 2 * self.w[self.n_vis:, :self.n_vis].dot(data) + self.b[self.n_vis:]\r\n",
    "                J_eff = self.w[self.n_vis:, self.n_vis:]\r\n",
    "\r\n",
    "                samp = np.append(data, self.sample(J_eff, b_eff))\r\n",
    "                outer_avg = np.outer(samp, samp)\r\n",
    "                np.fill_diagonal(outer_avg, 0)\r\n",
    "\r\n",
    "                pos_phase_w += outer_avg / batch_size\r\n",
    "                pos_phase_b += samp / batch_size\r\n",
    "                \r\n",
    "            samp = self.sample(self.w, self.b)\r\n",
    "            outer_avg = np.outer(samp, samp)\r\n",
    "            np.fill_diagonal(outer_avg, 0)\r\n",
    "            neg_phase_w = outer_avg\r\n",
    "            neg_phase_b = samp\r\n",
    "\r\n",
    "            self.w += lr * (pos_phase_w - neg_phase_w)\r\n",
    "            self.b += lr * (pos_phase_b - neg_phase_b)\r\n",
    "\r\n",
    "            # if epoch % epoch_drop == (epoch_drop - 1):\r\n",
    "                \r\n",
    "            #     sample_v = v\r\n",
    "            #     prob_sample_h = sigmoid(self.hidden_bias + np.dot(v, self.w))\r\n",
    "            #     sample_h = (np.random.rand(len(self.hidden_bias)) < prob_sample_h).astype(int)\r\n",
    "            #     prob_sample_v_out = sigmoid(self.visible_bias + np.dot(sample_h, self.w.T))\r\n",
    "            #     sample_output = (np.random.rand(len(self.visible_bias)) < prob_sample_v_out).astype(int)\r\n",
    "            #     learning_curve_plot.append(np.sum((np.array(v) - np.array(sample_output)) ** 2))\r\n",
    "\r\n",
    "            #     #learning_rate_decay\r\n",
    "            #     lr *= (1 - lr_decay)\r\n",
    "            #     print(\"lr = \", lr)\r\n",
    "\r\n",
    "        # plt.figure()\r\n",
    "        # plt.plot(learning_curve_plot)\r\n",
    "        # plt.xlabel('epoch')\r\n",
    "        # plt.ylabel('normalised MSE')\r\n",
    "        # plt.show()\r\n",
    "        return\r\n",
    "\r\n",
    "    # def generate(self, test_img=None): # just use sample\r\n",
    "    #     sample_v = []\r\n",
    "    #     if test_img == None:\r\n",
    "    #         sample_v = (np.random.rand(len(self.visible_bias)) < self.visible_bias).astype(int)\r\n",
    "    #         # print(\"sample_v: \", sample_v)\r\n",
    "    #         # print(\"visible_bias: \", self.visible_bias)\r\n",
    "    #     else:\r\n",
    "    #         sample_v = test_img\r\n",
    "    #     prob_h = sigmoid(self.hidden_bias + np.dot(sample_v, self.w))\r\n",
    "    #     sample_h = (np.random.rand(len(self.hidden_bias)) < prob_h).astype(int)\r\n",
    "\r\n",
    "    #     prob_v_out = sigmoid(self.visible_bias + np.dot(sample_h, self.w.T))\r\n",
    "    #     v_out = (np.random.rand(len(self.visible_bias)) < prob_v_out).astype(int)\r\n",
    "\r\n",
    "    #     return v_out\r\n",
    "\r\n",
    "    # def evaluate(self, result, test_img=None):\r\n",
    "    #     # sample_output = self.generate(test_img=test_img)\r\n",
    "    #     min_sum = 1000000\r\n",
    "    #     for pic in self.result_picture_tab:\r\n",
    "    #         new_sum = np.sum((np.array(result) - np.array(pic)) ** 2)\r\n",
    "    #         if new_sum < min_sum:\r\n",
    "    #             min_sum = new_sum\r\n",
    "\r\n",
    "    #     return min_sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class SRQBM(QBM): # in construction\r\n",
    "    \"\"\"\r\n",
    "    A semi-restricted quantum Boltzmann machine subclass. Connections in the hidden layer are forbidden.\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, *args, **kwargs):\r\n",
    "        super().__init__(*args, **kwargs)\r\n",
    "        self.w[self.n_vis:, self.n_vis:] = 0 # w in the hidden layer is forbidden\r\n",
    "\r\n",
    "    def train(self, training_dataset, epochs=1, batches=10, lr=0.11, lr_decay=0.1, epoch_drop=None):\r\n",
    "        \"\"\"\r\n",
    "        Minimize the upper bound of likelihood function.\r\n",
    "        Updates after each batch. Runs over the whole dataset after each epoch.\r\n",
    "        \"\"\"\r\n",
    "        # learning_curve_plot = []\r\n",
    "        batch_size = len(training_dataset) // batches\r\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\r\n",
    "\r\n",
    "        # if epoch_drop is None:\r\n",
    "        #     epoch_drop = epochs // 5\r\n",
    "\r\n",
    "        for batch in dataloader:\r\n",
    "            pos_phase_w = np.zeros((self.n, self.n))\r\n",
    "            pos_phase_b = np.zeros(self.n)\r\n",
    "            for data in batch: # parallelizable\r\n",
    "                b_eff = 2 * self.w[self.n_vis:, :self.n_vis].dot(data) + self.b[self.n_vis:] # for hidden units\r\n",
    "                D = np.sqrt(self.Gamma**2 + b_eff**2)\r\n",
    "\r\n",
    "                avg = np.append(data, b_eff/D * np.tanh(D))\r\n",
    "                outer_avg = np.outer(avg, avg)\r\n",
    "\r\n",
    "                pos_phase_w += outer_avg / batch_size\r\n",
    "                pos_phase_b += avg / batch_size\r\n",
    "                \r\n",
    "            samp = self.sample(self.w, self.b)\r\n",
    "            outer_avg = np.outer(samp, samp)\r\n",
    "            np.fill_diagonal(outer_avg, 0)\r\n",
    "            neg_phase_w = outer_avg\r\n",
    "            neg_phase_b = samp\r\n",
    "\r\n",
    "            self.w += lr * (pos_phase_w - neg_phase_w)\r\n",
    "            self.b += lr * (pos_phase_b - neg_phase_b)\r\n",
    "            self.w[self.n_vis:, self.n_vis:] = 0 # w in the hidden layer is forbidden\r\n",
    "\r\n",
    "            # if epoch % epoch_drop == (epoch_drop - 1):\r\n",
    "                \r\n",
    "            #     sample_v = v\r\n",
    "            #     prob_sample_h = sigmoid(self.hidden_bias + np.dot(v, self.w))\r\n",
    "            #     sample_h = (np.random.rand(len(self.hidden_bias)) < prob_sample_h).astype(int)\r\n",
    "            #     prob_sample_v_out = sigmoid(self.visible_bias + np.dot(sample_h, self.w.T))\r\n",
    "            #     sample_output = (np.random.rand(len(self.visible_bias)) < prob_sample_v_out).astype(int)\r\n",
    "            #     learning_curve_plot.append(np.sum((np.array(v) - np.array(sample_output)) ** 2))\r\n",
    "\r\n",
    "            #     #learning_rate_decay\r\n",
    "            #     lr *= (1 - lr_decay)\r\n",
    "            #     print(\"lr = \", lr)\r\n",
    "\r\n",
    "        # plt.figure()\r\n",
    "        # plt.plot(learning_curve_plot)\r\n",
    "        # plt.xlabel('epoch')\r\n",
    "        # plt.ylabel('normalised MSE')\r\n",
    "        # plt.show()\r\n",
    "        return\r\n",
    "\r\n",
    "    # def generate(self, test_img=None):\r\n",
    "    #     sample_v = []\r\n",
    "    #     if test_img == None:\r\n",
    "    #         sample_v = (np.random.rand(len(self.visible_bias)) < self.visible_bias).astype(int)\r\n",
    "    #         # print(\"sample_v: \", sample_v)\r\n",
    "    #         # print(\"visible_bias: \", self.visible_bias)\r\n",
    "    #     else:\r\n",
    "    #         sample_v = test_img\r\n",
    "    #     prob_h = sigmoid(self.hidden_bias + np.dot(sample_v, self.w))\r\n",
    "    #     sample_h = (np.random.rand(len(self.hidden_bias)) < prob_h).astype(int)\r\n",
    "\r\n",
    "    #     prob_v_out = sigmoid(self.visible_bias + np.dot(sample_h, self.w.T))\r\n",
    "    #     v_out = (np.random.rand(len(self.visible_bias)) < prob_v_out).astype(int)\r\n",
    "\r\n",
    "    #     return v_out\r\n",
    "\r\n",
    "    # def evaluate(self, result, test_img=None):\r\n",
    "    #     # sample_output = self.generate(test_img=test_img)\r\n",
    "    #     min_sum = 1000000\r\n",
    "    #     for pic in self.result_picture_tab:\r\n",
    "    #         new_sum = np.sum((np.array(result) - np.array(pic)) ** 2)\r\n",
    "    #         if new_sum < min_sum:\r\n",
    "    #             min_sum = new_sum\r\n",
    "\r\n",
    "    #     return min_sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "def sigmoid(x):\r\n",
    "    result = 1 / (1 + np.exp(-x))\r\n",
    "    return result\r\n",
    "\r\n",
    "class classicalRBM:\r\n",
    "    def __init__(self,\r\n",
    "                 n_vis,\r\n",
    "                 n_hid,\r\n",
    "                 err_function='mse',\r\n",
    "                 use_tqdm=True,\r\n",
    "                 tqdm=None):\r\n",
    "\r\n",
    "        if err_function not in {'mse', 'cosine'}:\r\n",
    "            raise ValueError('err_function should be either \\'mse\\' or \\'cosine\\'')\r\n",
    "\r\n",
    "        self._use_tqdm = use_tqdm\r\n",
    "        self._tqdm = None\r\n",
    "\r\n",
    "        if use_tqdm or tqdm is not None:\r\n",
    "            from tqdm import tqdm\r\n",
    "            self._tqdm = tqdm\r\n",
    "\r\n",
    "        self.n_vis = n_vis\r\n",
    "        self.n_hid = n_hid\r\n",
    "\r\n",
    "        self.w = (np.random.rand(self.n_vis, self.n_hid) * 2 - 1) * 1\r\n",
    "        self.visible_bias = (np.random.rand(self.n_vis) * 2 - 1) * 1\r\n",
    "        self.hidden_bias = (np.random.rand(self.n_hid) * 2 - 1) * 1\r\n",
    "\r\n",
    "        self.n_epoch = 0\r\n",
    "\r\n",
    "    def get_weights(self):\r\n",
    "        return self.w, \\\r\n",
    "               self.visible_bias, \\\r\n",
    "               self.hidden_bias\r\n",
    "\r\n",
    "    def set_weights(self, w, visible_bias, hidden_bias):\r\n",
    "        self.w = w\r\n",
    "        self.visible_bias = visible_bias\r\n",
    "        self.hidden_bias = hidden_bias\r\n",
    "\r\n",
    "    def get_Z(self):\r\n",
    "        Z = np.sum(np.exp(-1 * self.energies))\r\n",
    "        self.Z = Z\r\n",
    "        return Z\r\n",
    "\r\n",
    "    def train(self, training_data, len_x=1, len_y=1, epochs=1, lr=0.11, lr_decay=0.1, epoch_drop = None):\r\n",
    "        \"\"\"\r\n",
    "            maximize the product of probabilities assigned to some training set V\r\n",
    "            optimize the weight vector\r\n",
    "\r\n",
    "            single-step contrastive divergence (CD-1):\r\n",
    "            1. Take a training sample v,\r\n",
    "                compute the probabilities of the hidden units and\r\n",
    "                sample a hidden activation vector h from this probability distribution.\r\n",
    "            2. Compute the outer product of v and h and call this the positive gradient.\r\n",
    "            3. From h, sample a reconstruction v' of the visible units,\r\n",
    "                then resample the hidden activations h' from this. (Gibbs sampling step)\r\n",
    "            4. Compute the outer product of v' and h' and call this the negative gradient.\r\n",
    "            5. Let the update to the weight matrix W be the positive gradient minus the negative gradient,\r\n",
    "                times some learning rate\r\n",
    "            6. Update the biases a and b analogously: a=epsilon (v-v'), b=epsilon (h-h')\r\n",
    "\r\n",
    "            https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine\r\n",
    "        \"\"\"\r\n",
    "        learning_curve_plot = []\r\n",
    "        if epoch_drop == None:\r\n",
    "            epoch_drop = epochs / 5\r\n",
    "\r\n",
    "        for epoch in self.tqdm(range(epochs)):\r\n",
    "            # single step\r\n",
    "            # print(\"Training data len\", len(training_data))\r\n",
    "\r\n",
    "            # 1\r\n",
    "            # 1.1 Take a training sample v\r\n",
    "            random_selected_training_data_idx = epoch % len(training_data)\r\n",
    "            # print(\"selected_training_data_idx: \", random_selected_training_data_idx)\r\n",
    "\r\n",
    "            v = training_data[random_selected_training_data_idx]\r\n",
    "            # print(\"v: \", v)\r\n",
    "\r\n",
    "            # # 1.2 compute the probabilities of the hidden units\r\n",
    "            prob_h = sigmoid(self.hidden_bias + np.dot(v, self.w))\r\n",
    "\r\n",
    "            # h należy teraz wysamplować z tych wartości prawdopodobieństw\r\n",
    "            #klasycznie mogę to zrobić tak:\r\n",
    "            # print(\"self.hidden_bias: \", self.hidden_bias)\r\n",
    "            # print(\"np.dot(v, self.w): \", np.dot(v, self.w))\r\n",
    "            # print(\"self.hidden_bias + np.sum(np.dot(v, self.w)): \", self.hidden_bias + np.dot(v, self.w))\r\n",
    "            # print(\"prob_h: \", prob_h)\r\n",
    "            h = (np.random.rand(len(self.hidden_bias)) < prob_h).astype(int)\r\n",
    "            # print(\"h: \", h)\r\n",
    "\r\n",
    "            # 2 Compute the outer product of v and h and call this the positive gradient.\r\n",
    "            pos_grad = np.outer(v, h)\r\n",
    "            # print(\"pos_grad:\", pos_grad)\r\n",
    "\r\n",
    "            # 3\r\n",
    "            # 3.1 From h, sample a reconstruction v' of the visible units,\r\n",
    "            prob_v_prim = sigmoid(self.visible_bias + np.dot(h, self.w.T))\r\n",
    "            #znów klasycznie\r\n",
    "            v_prim = (np.random.rand(len(self.visible_bias)) < prob_v_prim).astype(int)\r\n",
    "            # print(\"v_prim: \", v_prim)\r\n",
    "\r\n",
    "            # 3.2 then resample the hidden activations h' from this. (Gibbs sampling step)\r\n",
    "            prob_h_prim = sigmoid(self.hidden_bias + np.dot(v_prim, self.w))\r\n",
    "            # h należy teraz wysamplować z tych wartości prawdopodobieństw\r\n",
    "            # klasycznie mogę to zrobić tak:\r\n",
    "            h_prim = (np.random.rand(len(self.hidden_bias)) < prob_h_prim).astype(int)\r\n",
    "            # print(\"h_prim: \", h_prim)\r\n",
    "\r\n",
    "            # 4 Compute the outer product of v' and h' and call this the negative gradient.\r\n",
    "            neg_grad = np.outer(v_prim, h_prim)\r\n",
    "            # print(\"neg_grad:\", neg_grad)\r\n",
    "\r\n",
    "            # 5 Let the update to the weight matrix W be the positive gradient minus the negative gradient,\r\n",
    "            #        times some learning rate\r\n",
    "            self.w += lr * (pos_grad - neg_grad)\r\n",
    "            # print(\"w: \", self.w)\r\n",
    "\r\n",
    "            # 6 Update the biases a and b analogously: a=epsilon (v-v'), b=epsilon (h-h')\r\n",
    "            self.visible_bias += lr * (np.array(v) - np.array(v_prim))\r\n",
    "            self.hidden_bias += lr * (np.array(h) - np.array(h_prim))\r\n",
    "            # print(\"visible_bias: \", self.visible_bias)\r\n",
    "            # print(\"hidden_bias: \", self.hidden_bias)\r\n",
    "\r\n",
    "            if epoch % epoch_drop == (epoch_drop - 1):\r\n",
    "                # krzywa uczenia\r\n",
    "                sample_v = v\r\n",
    "                prob_sample_h = sigmoid(self.hidden_bias + np.dot(v, self.w))\r\n",
    "                sample_h = (np.random.rand(len(self.hidden_bias)) < prob_sample_h).astype(int)\r\n",
    "                prob_sample_v_out = sigmoid(self.visible_bias + np.dot(sample_h, self.w.T))\r\n",
    "                sample_output = (np.random.rand(len(self.visible_bias)) < prob_sample_v_out).astype(int)\r\n",
    "                learning_curve_plot.append(np.sum((np.array(v) - np.array(sample_output)) ** 2))\r\n",
    "\r\n",
    "                #learning_rate_decay\r\n",
    "                lr *= (1 - lr_decay)\r\n",
    "                print(\"lr = \", lr)\r\n",
    "\r\n",
    "\r\n",
    "        # koniec\r\n",
    "        plt.figure()\r\n",
    "        plt.plot(learning_curve_plot)\r\n",
    "        plt.xlabel('epoch')\r\n",
    "        plt.ylabel('normalised MSE')\r\n",
    "        plt.show()\r\n",
    "        return\r\n",
    "\r\n",
    "    def generate(self, test_img=None):\r\n",
    "        sample_v = []\r\n",
    "        if test_img == None:\r\n",
    "            sample_v = (np.random.rand(len(self.visible_bias)) < self.visible_bias).astype(int)\r\n",
    "            # print(\"sample_v: \", sample_v)\r\n",
    "            # print(\"visible_bias: \", self.visible_bias)\r\n",
    "        else:\r\n",
    "            sample_v = test_img\r\n",
    "        prob_h = sigmoid(self.hidden_bias + np.dot(sample_v, self.w))\r\n",
    "        sample_h = (np.random.rand(len(self.hidden_bias)) < prob_h).astype(int)\r\n",
    "\r\n",
    "        prob_v_out = sigmoid(self.visible_bias + np.dot(sample_h, self.w.T))\r\n",
    "        v_out = (np.random.rand(len(self.visible_bias)) < prob_v_out).astype(int)\r\n",
    "\r\n",
    "        return v_out\r\n",
    "\r\n",
    "    def evaluate(self, result, test_img=None):\r\n",
    "        # sample_output = self.generate(test_img=test_img)\r\n",
    "        min_sum = 1000000\r\n",
    "        for pic in self.result_picture_tab:\r\n",
    "            new_sum = np.sum((np.array(result) - np.array(pic)) ** 2)\r\n",
    "            if new_sum < min_sum:\r\n",
    "                min_sum = new_sum\r\n",
    "\r\n",
    "        return min_sum"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('ml': conda)"
  },
  "interpreter": {
   "hash": "04a20cc0f25f2654a5fc5715c026c4c293afbc25926c593c301cab56769943bf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}