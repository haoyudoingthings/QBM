{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook is based on [Quantum Boltzmann Machine (M. H. Amin, et al., 2018)](https://journals.aps.org/prx/abstract/10.1103/PhysRevX.8.021050).\r\n",
    "\r\n",
    "Code is referenced from [mareksubocz/QRBM](https://github.com/mareksubocz/QRBM)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def sigmoid(x):\r\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dataset = tf.data.Dataset.range(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "list(dataset.as_numpy_iterator())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "list(dataset.shuffle(5, reshuffle_each_iteration=True).repeat(2).batch(3).as_numpy_iterator())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([1, 3, 4], dtype=int64),\n",
       " array([0, 2, 4], dtype=int64),\n",
       " array([3, 1, 0], dtype=int64),\n",
       " array([2], dtype=int64)]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "N = 5\r\n",
    "modes = 3\r\n",
    "p = 0.9\r\n",
    "dataset_size = 100\r\n",
    "\r\n",
    "center_lst = []\r\n",
    "dataset_lst = []\r\n",
    "for mode in modes:\r\n",
    "    s = np.random.binomial(1, 0.5, N)\r\n",
    "    center_lst.append(s)\r\n",
    "    dataset_lst.append([])\r\n",
    "    for _ in dataset_size:\r\n",
    "        data = (np.random.binomial(1, p, N) == s)\r\n",
    "        dataset_lst[-1].append(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quantum Boltzmann Machine"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class QBM:\r\n",
    "    \"\"\"\r\n",
    "    A general quantum Boltzmann machine class. No restrictions on the connection between nodes.\r\n",
    "    Variables are binary (\\pm 1).\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self,\r\n",
    "                 n_vis,\r\n",
    "                 n_hid,\r\n",
    "                 sampler='QMC',\r\n",
    "                 Gamma=0.1,\r\n",
    "                 err_function='mse',\r\n",
    "                 use_tqdm=True,\r\n",
    "                 tqdm=None):\r\n",
    "\r\n",
    "        if err_function not in {'mse', 'cosine'}:\r\n",
    "            raise ValueError('err_function should be either \\'mse\\' or \\'cosine\\'')\r\n",
    "\r\n",
    "        # progress bar\r\n",
    "        self._use_tqdm = use_tqdm\r\n",
    "        self._tqdm = None\r\n",
    "\r\n",
    "        if use_tqdm or tqdm is not None:\r\n",
    "            from tqdm import tqdm\r\n",
    "            self._tqdm = tqdm\r\n",
    "\r\n",
    "        self.n_vis = n_vis\r\n",
    "        self.n_hid = n_hid\r\n",
    "        self.n = n_vis + n_hid\r\n",
    "        self.sampler = sampler\r\n",
    "        self.Gamma = Gamma\r\n",
    "\r\n",
    "        # Convention:\r\n",
    "        # varibales 0 to (n_vis - 1) represent visible nodes;\r\n",
    "        # variables n_vis to (n - 1) represent hidden nodes;\r\n",
    "        # energy = -w_v.dot(visible).dot(visible) - w_h.dot(hidden).dot(hidden) - 2 * w_vh.dot(visible).dot(hidden)\r\n",
    "        #        = -w.dot(nodes).dot(nodes) - b.dot(nodes)\r\n",
    "        # self.w_v = np.random.uniform(-1, 1, (self.n_vis, self.n_vis))\r\n",
    "        # self.w_v = 0.5 * (self.w_v + self.w_v.T)\r\n",
    "        # self.w_h = np.random.uniform(-1, 1, (self.n_hid, self.n_hid))\r\n",
    "        # self.w_h = 0.5 * (self.w_h + self.w_h.T)\r\n",
    "        # self.w_vh = np.random.uniform(-1, 1, (self.n_vis, self.n_hid))\r\n",
    "        self.w = np.random.uniform(-1, 1, (self.n, self.n))\r\n",
    "        self.w = 0.5 * (self.w + self.w.T) # symmetric\r\n",
    "        np.fill_diagonal(self.w, 0)\r\n",
    "        self.b = np.random.uniform(-1, 1, self.n)\r\n",
    "\r\n",
    "        self.n_epoch = 0\r\n",
    "\r\n",
    "    def get_weights(self):\r\n",
    "        return self.w, self.b\r\n",
    "\r\n",
    "    def set_weights(self, w, b):\r\n",
    "        self.w = 0.5 * (w + w.T)\r\n",
    "        self.b = b\r\n",
    "\r\n",
    "    def get_Z(self):\r\n",
    "        \"\"\"\r\n",
    "        Return the canonical partition function\r\n",
    "        \"\"\"\r\n",
    "        Z = np.sum(np.exp(-self.energies))\r\n",
    "        self.Z = Z\r\n",
    "        return Z\r\n",
    "\r\n",
    "    def sample(self, J, num=10, Gamma0=5, M=10, T=0.05):\r\n",
    "        \"\"\"\r\n",
    "        Sample from the Boltzmann distribution of energy = -Q.dot(state).dot(state) and average them out\r\n",
    "        \"\"\"\r\n",
    "        if self.sampler == 'QMC' or self.sampler == 'SQA':\r\n",
    "            from sqa import one_SQA_run\r\n",
    "            \r\n",
    "            trans_fld_sched = np.linspace(Gamma0, self.Gamma, num=100)\r\n",
    "\r\n",
    "            results = []\r\n",
    "            for _ in num:\r\n",
    "                results.append(one_SQA_run(J, trans_fld_sched, M, T))\r\n",
    "            \r\n",
    "            return np.sum(np.array(results), axis=0) / num\r\n",
    "\r\n",
    "    def train(self, training_data, epochs=1, batches=10, lr=0.11, lr_decay=0.1, epoch_drop=None):\r\n",
    "        \"\"\"\r\n",
    "        Minimizes the upper bound of likelihood function.\r\n",
    "        Updates after each batch. Runs over the whole dataset after each epoch.\r\n",
    "        \"\"\"\r\n",
    "        # learning_curve_plot = []\r\n",
    "        batch_size = len(training_data) // batches\r\n",
    "\r\n",
    "        # if epoch_drop is None:\r\n",
    "        #     epoch_drop = epochs // 5\r\n",
    "\r\n",
    "        for batch in training_data.shuffle(len(training_data), reshuffle_each_iteration=True).repeat(epochs).batch(batch_size):\r\n",
    "\r\n",
    "            pos_phase_w = np.zeros((self.n, self.n))\r\n",
    "            pos_phase_b = np.zeros(self.n)\r\n",
    "            for data in batch: # parallelizable\r\n",
    "                # offset = self.w[:self.n_vis, :self.n_vis].dot(data).dot(data) + self.b[:self.n_vis].dot(data)\r\n",
    "                b_eff = 2 * self.w[self.n_vis:, :self.n_vis].dot(data) + self.b[self.n_vis:]\r\n",
    "                J_eff = self.w[self.n_vis:, self.n_vis:]\r\n",
    "\r\n",
    "                samp = data.extend(self.sample(J_eff + np.diag(b_eff)))\r\n",
    "                outer_avg = np.outer(samp, samp)\r\n",
    "                np.fill_diagonal(outer_avg, 0)\r\n",
    "\r\n",
    "                pos_phase_w += outer_avg / batch_size\r\n",
    "                pos_phase_b += samp / batch_size\r\n",
    "                \r\n",
    "            samp = self.sample(self.w + np.diag(self.b))\r\n",
    "            outer_avg = np.outer(samp, samp)\r\n",
    "            np.fill_diagonal(outer_avg, 0)\r\n",
    "            neg_phase_w = outer_avg\r\n",
    "            neg_phase_b = samp\r\n",
    "\r\n",
    "            self.w += lr * (pos_phase_w - neg_phase_w)\r\n",
    "            self.b += lr * (pos_phase_b - neg_phase_b)\r\n",
    "\r\n",
    "            # if epoch % epoch_drop == (epoch_drop - 1):\r\n",
    "                \r\n",
    "            #     sample_v = v\r\n",
    "            #     prob_sample_h = sigmoid(self.hidden_bias + np.dot(v, self.w))\r\n",
    "            #     sample_h = (np.random.rand(len(self.hidden_bias)) < prob_sample_h).astype(int)\r\n",
    "            #     prob_sample_v_out = sigmoid(self.visible_bias + np.dot(sample_h, self.w.T))\r\n",
    "            #     sample_output = (np.random.rand(len(self.visible_bias)) < prob_sample_v_out).astype(int)\r\n",
    "            #     learning_curve_plot.append(np.sum((np.array(v) - np.array(sample_output)) ** 2))\r\n",
    "\r\n",
    "            #     #learning_rate_decay\r\n",
    "            #     lr *= (1 - lr_decay)\r\n",
    "            #     print(\"lr = \", lr)\r\n",
    "\r\n",
    "        # plt.figure()\r\n",
    "        # plt.plot(learning_curve_plot)\r\n",
    "        # plt.xlabel('epoch')\r\n",
    "        # plt.ylabel('normalised MSE')\r\n",
    "        # plt.show()\r\n",
    "        return\r\n",
    "\r\n",
    "    def generate(self, test_img=None): # in construction\r\n",
    "        sample_v = []\r\n",
    "        if test_img == None:\r\n",
    "            sample_v = (np.random.rand(len(self.visible_bias)) < self.visible_bias).astype(int)\r\n",
    "            # print(\"sample_v: \", sample_v)\r\n",
    "            # print(\"visible_bias: \", self.visible_bias)\r\n",
    "        else:\r\n",
    "            sample_v = test_img\r\n",
    "        prob_h = sigmoid(self.hidden_bias + np.dot(sample_v, self.w))\r\n",
    "        sample_h = (np.random.rand(len(self.hidden_bias)) < prob_h).astype(int)\r\n",
    "\r\n",
    "        prob_v_out = sigmoid(self.visible_bias + np.dot(sample_h, self.w.T))\r\n",
    "        v_out = (np.random.rand(len(self.visible_bias)) < prob_v_out).astype(int)\r\n",
    "\r\n",
    "        return v_out\r\n",
    "\r\n",
    "    # def evaluate(self, result, test_img=None):\r\n",
    "    #     # sample_output = self.generate(test_img=test_img)\r\n",
    "    #     min_sum = 1000000\r\n",
    "    #     for pic in self.result_picture_tab:\r\n",
    "    #         new_sum = np.sum((np.array(result) - np.array(pic)) ** 2)\r\n",
    "    #         if new_sum < min_sum:\r\n",
    "    #             min_sum = new_sum\r\n",
    "\r\n",
    "    #     return min_sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class SRQBM(QBM): # in construction\r\n",
    "    \"\"\"\r\n",
    "    A semi-restricted quantum Boltzmann machine subclass. Connections in the hidden layer are forbidden.\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, *args, **kwargs):\r\n",
    "        super().__init__(self, *args, **kwargs)\r\n",
    "        self.w[self.n_vis:, self.n_vis:] = 0 # w in the hidden layer is forbidden\r\n",
    "\r\n",
    "    def train(self, training_data, epochs=1, batches=10, lr=0.11, lr_decay=0.1, epoch_drop=None):\r\n",
    "        \"\"\"\r\n",
    "        Minimize the upper bound of likelihood function.\r\n",
    "        Updates after each batch. Runs over the whole dataset after each epoch.\r\n",
    "        \"\"\"\r\n",
    "        # learning_curve_plot = []\r\n",
    "        batch_size = len(training_data) // batches\r\n",
    "\r\n",
    "        # if epoch_drop is None:\r\n",
    "        #     epoch_drop = epochs // 5\r\n",
    "\r\n",
    "        for batch in training_data.shuffle(len(training_data), reshuffle_each_iteration=True).repeat(epochs).batch(batch_size):\r\n",
    "\r\n",
    "            pos_phase_w = np.zeros((self.n, self.n))\r\n",
    "            pos_phase_b = np.zeros(self.n)\r\n",
    "            for data in batch: # parallelizable\r\n",
    "                b_eff = 2 * self.w[self.n_vis:, :self.n_vis].dot(data) + self.b[self.n_vis:] # for hidden units\r\n",
    "                D = np.sqrt(self.Gamma**2 + b_eff**2)\r\n",
    "\r\n",
    "                avg = data.extend(b_eff/D * np.tanh(D))\r\n",
    "                outer_avg = np.outer(avg, avg)\r\n",
    "\r\n",
    "                pos_phase_w += outer_avg / batch_size\r\n",
    "                pos_phase_b += avg / batch_size\r\n",
    "                \r\n",
    "            samp = self.sample(self.w + np.diag(self.b))\r\n",
    "            outer_avg = np.outer(samp, samp)\r\n",
    "            np.fill_diagonal(outer_avg, 0)\r\n",
    "            neg_phase_w = outer_avg\r\n",
    "            neg_phase_b = samp\r\n",
    "\r\n",
    "            self.w += lr * (pos_phase_w - neg_phase_w)\r\n",
    "            self.b += lr * (pos_phase_b - neg_phase_b)\r\n",
    "            self.w[self.n_vis:, self.n_vis:] = 0 # w in the hidden layer is forbidden\r\n",
    "\r\n",
    "            # if epoch % epoch_drop == (epoch_drop - 1):\r\n",
    "                \r\n",
    "            #     sample_v = v\r\n",
    "            #     prob_sample_h = sigmoid(self.hidden_bias + np.dot(v, self.w))\r\n",
    "            #     sample_h = (np.random.rand(len(self.hidden_bias)) < prob_sample_h).astype(int)\r\n",
    "            #     prob_sample_v_out = sigmoid(self.visible_bias + np.dot(sample_h, self.w.T))\r\n",
    "            #     sample_output = (np.random.rand(len(self.visible_bias)) < prob_sample_v_out).astype(int)\r\n",
    "            #     learning_curve_plot.append(np.sum((np.array(v) - np.array(sample_output)) ** 2))\r\n",
    "\r\n",
    "            #     #learning_rate_decay\r\n",
    "            #     lr *= (1 - lr_decay)\r\n",
    "            #     print(\"lr = \", lr)\r\n",
    "\r\n",
    "        # plt.figure()\r\n",
    "        # plt.plot(learning_curve_plot)\r\n",
    "        # plt.xlabel('epoch')\r\n",
    "        # plt.ylabel('normalised MSE')\r\n",
    "        # plt.show()\r\n",
    "        return\r\n",
    "\r\n",
    "    def generate(self, test_img=None):\r\n",
    "        sample_v = []\r\n",
    "        if test_img == None:\r\n",
    "            sample_v = (np.random.rand(len(self.visible_bias)) < self.visible_bias).astype(int)\r\n",
    "            # print(\"sample_v: \", sample_v)\r\n",
    "            # print(\"visible_bias: \", self.visible_bias)\r\n",
    "        else:\r\n",
    "            sample_v = test_img\r\n",
    "        prob_h = sigmoid(self.hidden_bias + np.dot(sample_v, self.w))\r\n",
    "        sample_h = (np.random.rand(len(self.hidden_bias)) < prob_h).astype(int)\r\n",
    "\r\n",
    "        prob_v_out = sigmoid(self.visible_bias + np.dot(sample_h, self.w.T))\r\n",
    "        v_out = (np.random.rand(len(self.visible_bias)) < prob_v_out).astype(int)\r\n",
    "\r\n",
    "        return v_out\r\n",
    "\r\n",
    "    def evaluate(self, result, test_img=None):\r\n",
    "        # sample_output = self.generate(test_img=test_img)\r\n",
    "        min_sum = 1000000\r\n",
    "        for pic in self.result_picture_tab:\r\n",
    "            new_sum = np.sum((np.array(result) - np.array(pic)) ** 2)\r\n",
    "            if new_sum < min_sum:\r\n",
    "                min_sum = new_sum\r\n",
    "\r\n",
    "        return min_sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sigmoid(x):\r\n",
    "    result = 1 / (1 + np.exp(-x))\r\n",
    "    return result\r\n",
    "\r\n",
    "class classicalRBM:\r\n",
    "    def __init__(self,\r\n",
    "                 n_vis,\r\n",
    "                 n_hid,\r\n",
    "                 err_function='mse',\r\n",
    "                 use_tqdm=True,\r\n",
    "                 tqdm=None):\r\n",
    "\r\n",
    "        if err_function not in {'mse', 'cosine'}:\r\n",
    "            raise ValueError('err_function should be either \\'mse\\' or \\'cosine\\'')\r\n",
    "\r\n",
    "        self._use_tqdm = use_tqdm\r\n",
    "        self._tqdm = None\r\n",
    "\r\n",
    "        if use_tqdm or tqdm is not None:\r\n",
    "            from tqdm import tqdm\r\n",
    "            self._tqdm = tqdm\r\n",
    "\r\n",
    "        self.n_vis = n_vis\r\n",
    "        self.n_hid = n_hid\r\n",
    "\r\n",
    "        self.w = (np.random.rand(self.n_vis, self.n_hid) * 2 - 1) * 1\r\n",
    "        self.visible_bias = (np.random.rand(self.n_vis) * 2 - 1) * 1\r\n",
    "        self.hidden_bias = (np.random.rand(self.n_hid) * 2 - 1) * 1\r\n",
    "\r\n",
    "        self.n_epoch = 0\r\n",
    "\r\n",
    "    def get_weights(self):\r\n",
    "        return self.w, \\\r\n",
    "               self.visible_bias, \\\r\n",
    "               self.hidden_bias\r\n",
    "\r\n",
    "    def set_weights(self, w, visible_bias, hidden_bias):\r\n",
    "        self.w = w\r\n",
    "        self.visible_bias = visible_bias\r\n",
    "        self.hidden_bias = hidden_bias\r\n",
    "\r\n",
    "    def get_Z(self):\r\n",
    "        Z = np.sum(np.exp(-1 * self.energies))\r\n",
    "        self.Z = Z\r\n",
    "        return Z\r\n",
    "\r\n",
    "    def train(self, training_data, len_x=1, len_y=1, epochs=1, lr=0.11, lr_decay=0.1, epoch_drop = None):\r\n",
    "        \"\"\"\r\n",
    "            maximize the product of probabilities assigned to some training set V\r\n",
    "            optimize the weight vector\r\n",
    "\r\n",
    "            single-step contrastive divergence (CD-1):\r\n",
    "            1. Take a training sample v,\r\n",
    "                compute the probabilities of the hidden units and\r\n",
    "                sample a hidden activation vector h from this probability distribution.\r\n",
    "            2. Compute the outer product of v and h and call this the positive gradient.\r\n",
    "            3. From h, sample a reconstruction v' of the visible units,\r\n",
    "                then resample the hidden activations h' from this. (Gibbs sampling step)\r\n",
    "            4. Compute the outer product of v' and h' and call this the negative gradient.\r\n",
    "            5. Let the update to the weight matrix W be the positive gradient minus the negative gradient,\r\n",
    "                times some learning rate\r\n",
    "            6. Update the biases a and b analogously: a=epsilon (v-v'), b=epsilon (h-h')\r\n",
    "\r\n",
    "            https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine\r\n",
    "        \"\"\"\r\n",
    "        learning_curve_plot = []\r\n",
    "        if epoch_drop == None:\r\n",
    "            epoch_drop = epochs / 5\r\n",
    "\r\n",
    "        for epoch in self.tqdm(range(epochs)):\r\n",
    "            # single step\r\n",
    "            # print(\"Training data len\", len(training_data))\r\n",
    "\r\n",
    "            # 1\r\n",
    "            # 1.1 Take a training sample v\r\n",
    "            random_selected_training_data_idx = epoch % len(training_data)\r\n",
    "            # print(\"selected_training_data_idx: \", random_selected_training_data_idx)\r\n",
    "\r\n",
    "            v = training_data[random_selected_training_data_idx]\r\n",
    "            # print(\"v: \", v)\r\n",
    "\r\n",
    "            # # 1.2 compute the probabilities of the hidden units\r\n",
    "            prob_h = sigmoid(self.hidden_bias + np.dot(v, self.w))\r\n",
    "\r\n",
    "            # h należy teraz wysamplować z tych wartości prawdopodobieństw\r\n",
    "            #klasycznie mogę to zrobić tak:\r\n",
    "            # print(\"self.hidden_bias: \", self.hidden_bias)\r\n",
    "            # print(\"np.dot(v, self.w): \", np.dot(v, self.w))\r\n",
    "            # print(\"self.hidden_bias + np.sum(np.dot(v, self.w)): \", self.hidden_bias + np.dot(v, self.w))\r\n",
    "            # print(\"prob_h: \", prob_h)\r\n",
    "            h = (np.random.rand(len(self.hidden_bias)) < prob_h).astype(int)\r\n",
    "            # print(\"h: \", h)\r\n",
    "\r\n",
    "            # 2 Compute the outer product of v and h and call this the positive gradient.\r\n",
    "            pos_grad = np.outer(v, h)\r\n",
    "            # print(\"pos_grad:\", pos_grad)\r\n",
    "\r\n",
    "            # 3\r\n",
    "            # 3.1 From h, sample a reconstruction v' of the visible units,\r\n",
    "            prob_v_prim = sigmoid(self.visible_bias + np.dot(h, self.w.T))\r\n",
    "            #znów klasycznie\r\n",
    "            v_prim = (np.random.rand(len(self.visible_bias)) < prob_v_prim).astype(int)\r\n",
    "            # print(\"v_prim: \", v_prim)\r\n",
    "\r\n",
    "            # 3.2 then resample the hidden activations h' from this. (Gibbs sampling step)\r\n",
    "            prob_h_prim = sigmoid(self.hidden_bias + np.dot(v_prim, self.w))\r\n",
    "            # h należy teraz wysamplować z tych wartości prawdopodobieństw\r\n",
    "            # klasycznie mogę to zrobić tak:\r\n",
    "            h_prim = (np.random.rand(len(self.hidden_bias)) < prob_h_prim).astype(int)\r\n",
    "            # print(\"h_prim: \", h_prim)\r\n",
    "\r\n",
    "            # 4 Compute the outer product of v' and h' and call this the negative gradient.\r\n",
    "            neg_grad = np.outer(v_prim, h_prim)\r\n",
    "            # print(\"neg_grad:\", neg_grad)\r\n",
    "\r\n",
    "            # 5 Let the update to the weight matrix W be the positive gradient minus the negative gradient,\r\n",
    "            #        times some learning rate\r\n",
    "            self.w += lr * (pos_grad - neg_grad)\r\n",
    "            # print(\"w: \", self.w)\r\n",
    "\r\n",
    "            # 6 Update the biases a and b analogously: a=epsilon (v-v'), b=epsilon (h-h')\r\n",
    "            self.visible_bias += lr * (np.array(v) - np.array(v_prim))\r\n",
    "            self.hidden_bias += lr * (np.array(h) - np.array(h_prim))\r\n",
    "            # print(\"visible_bias: \", self.visible_bias)\r\n",
    "            # print(\"hidden_bias: \", self.hidden_bias)\r\n",
    "\r\n",
    "            if epoch % epoch_drop == (epoch_drop - 1):\r\n",
    "                # krzywa uczenia\r\n",
    "                sample_v = v\r\n",
    "                prob_sample_h = sigmoid(self.hidden_bias + np.dot(v, self.w))\r\n",
    "                sample_h = (np.random.rand(len(self.hidden_bias)) < prob_sample_h).astype(int)\r\n",
    "                prob_sample_v_out = sigmoid(self.visible_bias + np.dot(sample_h, self.w.T))\r\n",
    "                sample_output = (np.random.rand(len(self.visible_bias)) < prob_sample_v_out).astype(int)\r\n",
    "                learning_curve_plot.append(np.sum((np.array(v) - np.array(sample_output)) ** 2))\r\n",
    "\r\n",
    "                #learning_rate_decay\r\n",
    "                lr *= (1 - lr_decay)\r\n",
    "                print(\"lr = \", lr)\r\n",
    "\r\n",
    "\r\n",
    "        # koniec\r\n",
    "        plt.figure()\r\n",
    "        plt.plot(learning_curve_plot)\r\n",
    "        plt.xlabel('epoch')\r\n",
    "        plt.ylabel('normalised MSE')\r\n",
    "        plt.show()\r\n",
    "        return\r\n",
    "\r\n",
    "    def generate(self, test_img=None):\r\n",
    "        sample_v = []\r\n",
    "        if test_img == None:\r\n",
    "            sample_v = (np.random.rand(len(self.visible_bias)) < self.visible_bias).astype(int)\r\n",
    "            # print(\"sample_v: \", sample_v)\r\n",
    "            # print(\"visible_bias: \", self.visible_bias)\r\n",
    "        else:\r\n",
    "            sample_v = test_img\r\n",
    "        prob_h = sigmoid(self.hidden_bias + np.dot(sample_v, self.w))\r\n",
    "        sample_h = (np.random.rand(len(self.hidden_bias)) < prob_h).astype(int)\r\n",
    "\r\n",
    "        prob_v_out = sigmoid(self.visible_bias + np.dot(sample_h, self.w.T))\r\n",
    "        v_out = (np.random.rand(len(self.visible_bias)) < prob_v_out).astype(int)\r\n",
    "\r\n",
    "        return v_out\r\n",
    "\r\n",
    "    def evaluate(self, result, test_img=None):\r\n",
    "        # sample_output = self.generate(test_img=test_img)\r\n",
    "        min_sum = 1000000\r\n",
    "        for pic in self.result_picture_tab:\r\n",
    "            new_sum = np.sum((np.array(result) - np.array(pic)) ** 2)\r\n",
    "            if new_sum < min_sum:\r\n",
    "                min_sum = new_sum\r\n",
    "\r\n",
    "        return min_sum"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('ml': conda)"
  },
  "interpreter": {
   "hash": "04a20cc0f25f2654a5fc5715c026c4c293afbc25926c593c301cab56769943bf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}